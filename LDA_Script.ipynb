{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import contractions\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>category</th>\n",
       "      <th>air_date</th>\n",
       "      <th>question</th>\n",
       "      <th>value</th>\n",
       "      <th>answer</th>\n",
       "      <th>round</th>\n",
       "      <th>show_number</th>\n",
       "      <th>questionClean</th>\n",
       "      <th>categoryClean</th>\n",
       "      <th>combinedClean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>HISTORY</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>'For the last 8 years of his life, Galileo was...</td>\n",
       "      <td>200.0</td>\n",
       "      <td>Copernicus</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>4680</td>\n",
       "      <td>last 8 year life galileo house arrest espousin...</td>\n",
       "      <td>history</td>\n",
       "      <td>history last 8 year life galileo house arrest ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ESPN's TOP 10 ALL-TIME ATHLETES</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>'No. 2: 1912 Olympian; football star at Carlis...</td>\n",
       "      <td>200.0</td>\n",
       "      <td>Jim Thorpe</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>4680</td>\n",
       "      <td>2 1912 olympian football star carlisle indian ...</td>\n",
       "      <td>espn top 10 alltime athlete</td>\n",
       "      <td>espn top 10 alltime athlete 2 1912 olympian fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>EVERYBODY TALKS ABOUT IT...</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>'The city of Yuma in this state has a record a...</td>\n",
       "      <td>200.0</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>4680</td>\n",
       "      <td>city yuma state record average 4055 hour sunsh...</td>\n",
       "      <td>everybody talk</td>\n",
       "      <td>everybody talk city yuma state record average ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>THE COMPANY LINE</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>'In 1963, live on \"The Art Linkletter Show\", t...</td>\n",
       "      <td>200.0</td>\n",
       "      <td>McDonald\\'s</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>4680</td>\n",
       "      <td>1963 live art linkletter show company served b...</td>\n",
       "      <td>company line</td>\n",
       "      <td>company line 1963 live art linkletter show com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>EPITAPHS &amp; TRIBUTES</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Signer of the Declaration of Independence, fra...</td>\n",
       "      <td>200.0</td>\n",
       "      <td>John Adams</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>4680</td>\n",
       "      <td>signer declaration independence framer constit...</td>\n",
       "      <td>epitaph tribute</td>\n",
       "      <td>epitaph tribute signer declaration independenc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                         category    air_date  \\\n",
       "0           0                          HISTORY  2004-12-31   \n",
       "1           1  ESPN's TOP 10 ALL-TIME ATHLETES  2004-12-31   \n",
       "2           2      EVERYBODY TALKS ABOUT IT...  2004-12-31   \n",
       "3           3                 THE COMPANY LINE  2004-12-31   \n",
       "4           4              EPITAPHS & TRIBUTES  2004-12-31   \n",
       "\n",
       "                                            question  value       answer  \\\n",
       "0  'For the last 8 years of his life, Galileo was...  200.0   Copernicus   \n",
       "1  'No. 2: 1912 Olympian; football star at Carlis...  200.0   Jim Thorpe   \n",
       "2  'The city of Yuma in this state has a record a...  200.0      Arizona   \n",
       "3  'In 1963, live on \"The Art Linkletter Show\", t...  200.0  McDonald\\'s   \n",
       "4  Signer of the Declaration of Independence, fra...  200.0   John Adams   \n",
       "\n",
       "       round  show_number                                      questionClean  \\\n",
       "0  Jeopardy!         4680  last 8 year life galileo house arrest espousin...   \n",
       "1  Jeopardy!         4680  2 1912 olympian football star carlisle indian ...   \n",
       "2  Jeopardy!         4680  city yuma state record average 4055 hour sunsh...   \n",
       "3  Jeopardy!         4680  1963 live art linkletter show company served b...   \n",
       "4  Jeopardy!         4680  signer declaration independence framer constit...   \n",
       "\n",
       "                 categoryClean  \\\n",
       "0                      history   \n",
       "1  espn top 10 alltime athlete   \n",
       "2               everybody talk   \n",
       "3                 company line   \n",
       "4              epitaph tribute   \n",
       "\n",
       "                                       combinedClean  \n",
       "0  history last 8 year life galileo house arrest ...  \n",
       "1  espn top 10 alltime athlete 2 1912 olympian fo...  \n",
       "2  everybody talk city yuma state record average ...  \n",
       "3  company line 1963 live art linkletter show com...  \n",
       "4  epitaph tribute signer declaration independenc...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup file names\n",
    "inFileName = 'JeopardyQuestions_Clean-Numbers.csv'\n",
    "# Load cleaned dataset\n",
    "df = pd.read_csv(inFileName)\n",
    "# Look at data header\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(206407, 11)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at data shape\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NA values from the cleaned questions \n",
    "df.dropna(subset=['questionClean'], inplace=True)\n",
    "# Reset the dataframe index\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "# Create a list of documents to perform LDA on. Each document is a cleaned/preprocessed/lemmatized question\n",
    "docs = df['questionClean'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(206383, 11)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Review data shape after removing NA questions\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a collection of text documents to a matrix of token counts\n",
    "# 2000 unigrams and bigrams will be used as tokens for LDA topic modeling\n",
    "tf_vectorizer = CountVectorizer(max_features=2000, \n",
    "                                ngram_range=(1, 2),\n",
    "                                stop_words = 'english')\n",
    "# Vectorize the corpus of cleaned Jeopardy! questions. Creates document term matrix (tf)\n",
    "tf = tf_vectorizer.fit_transform(docs)\n",
    "# Get feature names (i.e., the 2000 unigrams and bigrams that will be used in the model)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of max_iter: 200\n",
      "iteration: 2 of max_iter: 200\n",
      "iteration: 3 of max_iter: 200\n",
      "iteration: 4 of max_iter: 200\n",
      "iteration: 5 of max_iter: 200\n",
      "iteration: 6 of max_iter: 200\n",
      "iteration: 7 of max_iter: 200\n",
      "iteration: 8 of max_iter: 200\n",
      "iteration: 9 of max_iter: 200\n",
      "iteration: 10 of max_iter: 200\n",
      "iteration: 11 of max_iter: 200\n",
      "iteration: 12 of max_iter: 200\n",
      "iteration: 13 of max_iter: 200\n",
      "iteration: 14 of max_iter: 200\n",
      "iteration: 15 of max_iter: 200\n",
      "iteration: 16 of max_iter: 200\n",
      "iteration: 17 of max_iter: 200\n",
      "iteration: 18 of max_iter: 200\n",
      "iteration: 19 of max_iter: 200\n",
      "iteration: 20 of max_iter: 200\n",
      "iteration: 21 of max_iter: 200\n",
      "iteration: 22 of max_iter: 200\n",
      "iteration: 23 of max_iter: 200\n",
      "iteration: 24 of max_iter: 200\n",
      "iteration: 25 of max_iter: 200\n",
      "iteration: 26 of max_iter: 200\n",
      "iteration: 27 of max_iter: 200\n",
      "iteration: 28 of max_iter: 200\n",
      "iteration: 29 of max_iter: 200\n",
      "iteration: 30 of max_iter: 200\n",
      "iteration: 31 of max_iter: 200\n",
      "iteration: 32 of max_iter: 200\n",
      "iteration: 33 of max_iter: 200\n",
      "iteration: 34 of max_iter: 200\n",
      "iteration: 35 of max_iter: 200\n",
      "iteration: 36 of max_iter: 200\n",
      "iteration: 37 of max_iter: 200\n",
      "iteration: 38 of max_iter: 200\n",
      "iteration: 39 of max_iter: 200\n",
      "iteration: 40 of max_iter: 200\n",
      "iteration: 41 of max_iter: 200\n",
      "iteration: 42 of max_iter: 200\n",
      "iteration: 43 of max_iter: 200\n",
      "iteration: 44 of max_iter: 200\n",
      "iteration: 45 of max_iter: 200\n",
      "iteration: 46 of max_iter: 200\n",
      "iteration: 47 of max_iter: 200\n",
      "iteration: 48 of max_iter: 200\n",
      "iteration: 49 of max_iter: 200\n",
      "iteration: 50 of max_iter: 200\n",
      "iteration: 51 of max_iter: 200\n",
      "iteration: 52 of max_iter: 200\n",
      "iteration: 53 of max_iter: 200\n",
      "iteration: 54 of max_iter: 200\n",
      "iteration: 55 of max_iter: 200\n",
      "iteration: 56 of max_iter: 200\n",
      "iteration: 57 of max_iter: 200\n",
      "iteration: 58 of max_iter: 200\n",
      "iteration: 59 of max_iter: 200\n",
      "iteration: 60 of max_iter: 200\n",
      "iteration: 61 of max_iter: 200\n",
      "iteration: 62 of max_iter: 200\n",
      "iteration: 63 of max_iter: 200\n",
      "iteration: 64 of max_iter: 200\n",
      "iteration: 65 of max_iter: 200\n",
      "iteration: 66 of max_iter: 200\n",
      "iteration: 67 of max_iter: 200\n",
      "iteration: 68 of max_iter: 200\n",
      "iteration: 69 of max_iter: 200\n",
      "iteration: 70 of max_iter: 200\n",
      "iteration: 71 of max_iter: 200\n",
      "iteration: 72 of max_iter: 200\n",
      "iteration: 73 of max_iter: 200\n",
      "iteration: 74 of max_iter: 200\n",
      "iteration: 75 of max_iter: 200\n",
      "iteration: 76 of max_iter: 200\n",
      "iteration: 77 of max_iter: 200\n",
      "iteration: 78 of max_iter: 200\n",
      "iteration: 79 of max_iter: 200\n",
      "iteration: 80 of max_iter: 200\n",
      "iteration: 81 of max_iter: 200\n",
      "iteration: 82 of max_iter: 200\n",
      "iteration: 83 of max_iter: 200\n",
      "iteration: 84 of max_iter: 200\n",
      "iteration: 85 of max_iter: 200\n",
      "iteration: 86 of max_iter: 200\n",
      "iteration: 87 of max_iter: 200\n",
      "iteration: 88 of max_iter: 200\n",
      "iteration: 89 of max_iter: 200\n",
      "iteration: 90 of max_iter: 200\n",
      "iteration: 91 of max_iter: 200\n",
      "iteration: 92 of max_iter: 200\n",
      "iteration: 93 of max_iter: 200\n",
      "iteration: 94 of max_iter: 200\n",
      "iteration: 95 of max_iter: 200\n",
      "iteration: 96 of max_iter: 200\n",
      "iteration: 97 of max_iter: 200\n",
      "iteration: 98 of max_iter: 200\n",
      "iteration: 99 of max_iter: 200\n",
      "iteration: 100 of max_iter: 200\n",
      "iteration: 101 of max_iter: 200\n",
      "iteration: 102 of max_iter: 200\n",
      "iteration: 103 of max_iter: 200\n",
      "iteration: 104 of max_iter: 200\n",
      "iteration: 105 of max_iter: 200\n",
      "iteration: 106 of max_iter: 200\n",
      "iteration: 107 of max_iter: 200\n",
      "iteration: 108 of max_iter: 200\n",
      "iteration: 109 of max_iter: 200\n",
      "iteration: 110 of max_iter: 200\n",
      "iteration: 111 of max_iter: 200\n",
      "iteration: 112 of max_iter: 200\n",
      "iteration: 113 of max_iter: 200\n",
      "iteration: 114 of max_iter: 200\n",
      "iteration: 115 of max_iter: 200\n",
      "iteration: 116 of max_iter: 200\n",
      "iteration: 117 of max_iter: 200\n",
      "iteration: 118 of max_iter: 200\n",
      "iteration: 119 of max_iter: 200\n",
      "iteration: 120 of max_iter: 200\n",
      "iteration: 121 of max_iter: 200\n",
      "iteration: 122 of max_iter: 200\n",
      "iteration: 123 of max_iter: 200\n",
      "iteration: 124 of max_iter: 200\n",
      "iteration: 125 of max_iter: 200\n",
      "iteration: 126 of max_iter: 200\n",
      "iteration: 127 of max_iter: 200\n",
      "iteration: 128 of max_iter: 200\n",
      "iteration: 129 of max_iter: 200\n",
      "iteration: 130 of max_iter: 200\n",
      "iteration: 131 of max_iter: 200\n",
      "iteration: 132 of max_iter: 200\n",
      "iteration: 133 of max_iter: 200\n",
      "iteration: 134 of max_iter: 200\n",
      "iteration: 135 of max_iter: 200\n",
      "iteration: 136 of max_iter: 200\n",
      "iteration: 137 of max_iter: 200\n",
      "iteration: 138 of max_iter: 200\n",
      "iteration: 139 of max_iter: 200\n",
      "iteration: 140 of max_iter: 200\n",
      "iteration: 141 of max_iter: 200\n",
      "iteration: 142 of max_iter: 200\n",
      "iteration: 143 of max_iter: 200\n",
      "iteration: 144 of max_iter: 200\n",
      "iteration: 145 of max_iter: 200\n",
      "iteration: 146 of max_iter: 200\n",
      "iteration: 147 of max_iter: 200\n",
      "iteration: 148 of max_iter: 200\n",
      "iteration: 149 of max_iter: 200\n",
      "iteration: 150 of max_iter: 200\n",
      "iteration: 151 of max_iter: 200\n",
      "iteration: 152 of max_iter: 200\n",
      "iteration: 153 of max_iter: 200\n",
      "iteration: 154 of max_iter: 200\n",
      "iteration: 155 of max_iter: 200\n",
      "iteration: 156 of max_iter: 200\n",
      "iteration: 157 of max_iter: 200\n",
      "iteration: 158 of max_iter: 200\n",
      "iteration: 159 of max_iter: 200\n",
      "iteration: 160 of max_iter: 200\n",
      "iteration: 161 of max_iter: 200\n",
      "iteration: 162 of max_iter: 200\n",
      "iteration: 163 of max_iter: 200\n",
      "iteration: 164 of max_iter: 200\n",
      "iteration: 165 of max_iter: 200\n",
      "iteration: 166 of max_iter: 200\n",
      "iteration: 167 of max_iter: 200\n",
      "iteration: 168 of max_iter: 200\n",
      "iteration: 169 of max_iter: 200\n",
      "iteration: 170 of max_iter: 200\n",
      "iteration: 171 of max_iter: 200\n",
      "iteration: 172 of max_iter: 200\n",
      "iteration: 173 of max_iter: 200\n",
      "iteration: 174 of max_iter: 200\n",
      "iteration: 175 of max_iter: 200\n",
      "iteration: 176 of max_iter: 200\n",
      "iteration: 177 of max_iter: 200\n",
      "iteration: 178 of max_iter: 200\n",
      "iteration: 179 of max_iter: 200\n",
      "iteration: 180 of max_iter: 200\n",
      "iteration: 181 of max_iter: 200\n",
      "iteration: 182 of max_iter: 200\n",
      "iteration: 183 of max_iter: 200\n",
      "iteration: 184 of max_iter: 200\n",
      "iteration: 185 of max_iter: 200\n",
      "iteration: 186 of max_iter: 200\n",
      "iteration: 187 of max_iter: 200\n",
      "iteration: 188 of max_iter: 200\n",
      "iteration: 189 of max_iter: 200\n",
      "iteration: 190 of max_iter: 200\n",
      "iteration: 191 of max_iter: 200\n",
      "iteration: 192 of max_iter: 200\n",
      "iteration: 193 of max_iter: 200\n",
      "iteration: 194 of max_iter: 200\n",
      "iteration: 195 of max_iter: 200\n",
      "iteration: 196 of max_iter: 200\n",
      "iteration: 197 of max_iter: 200\n",
      "iteration: 198 of max_iter: 200\n",
      "iteration: 199 of max_iter: 200\n",
      "iteration: 200 of max_iter: 200\n"
     ]
    }
   ],
   "source": [
    "# LDA topic model to discover 10 topics \n",
    "# Document term matrix (tf) is used to fit the model\n",
    "lda_10 = LatentDirichletAllocation(n_components=10, \n",
    "                                max_iter=200, \n",
    "                                learning_method='online', \n",
    "                                learning_offset=50.,\n",
    "                                random_state=0,\n",
    "                                verbose=1).fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build function to display topics \n",
    "# Code adapted from the following source: https://blog.mlreview.com/topic-modeling-with-scikit-learn-e80d33668730\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        # print \"Topic %d:\" % (topic_idx)\n",
    "        print(f'Topic {topic_idx}')\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0\n",
      "known novel say star group born character got home school end way english member red museum william story came large nickname named robert run tale poet march street want begin june prize force artist helped includes east thomas need 1994 highest product paris texas royal richard season writer senator giant open sitcom turned money 2006 start store developed christmas lived colony dutch master nobel stop bone mammal bank international met class final 1965 boston established ride bob pop stage lee martin religious future medical crime fan course 23 beer ohio detective instead near leg taken boy attack working 18th die\n",
      "Topic 1\n",
      "type time woman make company named life famous place man son water god long took brother popular men right mountain night high st served food introduced light plant hall help area fish charles month magazine horse 1997 dance live director prince originally 15 tom lord player according sun planet hold fame playing stone michael moon secretary hill louis baseball item chain doctor read minute wear 1963 edward process miss card started 1960 bar equal founder example range greek fight period joseph joe treat smith produce section sugar measure hall fame wwii chocolate 1945 explorer jackson holy source worker worn johnson trip\n",
      "Topic 2\n",
      "state united united state new island wrote day work number include mile wife york new york tell west james music capital color italian 1st published thing named ship left court flag history sister going featured opened blood bay famous mexico husband holiday honor room oldest brown coast living talk presidential florida bridge summer province pacific wood figure northern newspaper festival rose 1961 meat canada valley supreme justice contains republic case ground peninsula medal frank mom mount program territory prison pulitzer weapon painted massachusetts yellow greatest return mayor vote state president supreme court silver coin police folk york city beginning point shape 1946 state capital cape mississippi\n",
      "Topic 3\n",
      "man term seen author game british little nation death use classic small child town dog early built france film including kid main book bc album le london common comedy composer refers style starred office flower today peter original given metal dr jack ballet hotel invented painting drama ball oil male chinese object week 11 collection italy 1982 synonym 1979 1978 saw nicknamed colorful debut union hard scientist speech computer kill russia theme different secret rome playwright dark 2002 van board scene 1957 flow partner iii forest reach spent 1954 flight harry slang breed science report cold lion twin orange associated\n",
      "Topic 4\n",
      "city mean war hit people love founded gave black girl george actor general short singer named center spanish stand mother guy held hero air piece specie european told bear fall real million 30 field minister home job february san really lead adjective shot 14 paper 1990 sign dead list disease study prime congress civil port bad 2005 career club post asked face story located usa 1967 prime minister 1975 1987 16 branch jesus 2007 ring actually probably sent 1981 modern religion glass october pound murder society civil war 10 lincoln birth saying signed troop 1959 capital city ford eastern hear train nba disney\n",
      "Topic 5\n",
      "year country john city house largest old white body good later died began la england age went opera blue led july law african washington 20 january ii using 1995 chief late middle russian december inspired award military governor government 50 southern native human turn unit drink half better nyc virginia 13 2003 1986 europe china visit elected thought 1980 moved hour 1964 1968 captain ice soldier student free senate 1962 rule space winter mass palace 70 ended hope desert area designed cream bell 24 received population dc arthur milk 60 knight road soviet twice tried olympics white house japan year later egypt\n",
      "Topic 6\n",
      "called said play song line know meaning bird film musical best lake person queen language young version lady clue role 1996 poem ancient car april lost friend crew actress 1998 gold fruit let power party baby clue crew kind 2000 november mark indian named sold making 1991 pair egg news broadway ocean appeared arm golden voice video shakespeare model statue 25 germany eat lot canadian 1971 machine hair 1974 jimmy directed leaf structure 1970 word meaning traditional dad roll department substance shall boat certain sarah angel worked titled celebrated scottish skin elizabeth 1948 cast birthday double 19 1953 1952 wonder mind ray\n",
      "Topic 7\n",
      "title used american river south country book sea animal north letter america team head father park leader major sport land art feature married roman daughter win world boy point army element mary cover service share border church written region africa square emperor title character 1993 league wild cut 2004 india gas try organization degree 1969 garden 2009 18 cross 17 movement plane joined daily longest following greek 1984 picture ran hat considered running add form cell christian episode bestseller goddess sequel true national park beat standard bible billion western cup vegetable 90 restaurant wedding 1947 south american nearly navy olympic launched georgia duke\n",
      "Topic 8\n",
      "like king great century national family big university german rock oscar sound created battle tree form college band earth brand building green act away heart phrase eye killed near female paul hand heard 19th henry 1989 40 winner ____ hot cat central dish 2word think formed date favorite peace race spain insect sweet organ wine grand 19th century political business cheese produced special asian football saint radio material pope sir bowl wind precedes 80 spring described added meet important sound like 1956 base 1939 shared irish britain 1966 1955 chicken sauce gun changed beautiful tour 20th alexander door atlantic liquid surface buried\n",
      "Topic 9\n",
      "word president played film world come french tv capital movie series latin set foot based second mr greek 12 1999 event site look official variety david symbol lie order august usually california book record device ad discovered 10 sang 100 chicago 2001 county 2008 creature writing single japanese 1988 comic included wall legend public 1976 empire current 1985 1973 september instrument brought fly continent control independence change hi 1977 dream leading subject adam tower far beach travel fair 2010 action screen close vice walk size host fear singing follows annual speed jones followed shoe kingdom egyptian historic marriage soup test\n"
     ]
    }
   ],
   "source": [
    "# Display the first n tokens of each topic\n",
    "no_top_tokens = 100\n",
    "display_topics(lda_10, tf_feature_names, no_top_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to come up with phrases to describe each of these ten topics, we used the following prompt in ChatGPT:\n",
    "# Please come up with three word summaries that encapsulate the ideas in the following ten topics:\n",
    "# And then we pasted in the top 100 n tokens for each topic for ChatGPT to summarize.\n",
    "# The results are below: \n",
    "# Topic 0: \"Literary legacy, hometown stories, notable characters.\"\n",
    "# Topic 1: \"Life stories, cultural icons, family moments.\"\n",
    "# Topic 2: \"State history, geographical tales, cultural significance.\"\n",
    "# Topic 3: \"Artistic expressions, creative works, cultural impact.\"\n",
    "# Topic 4: \"City narratives, historical events, cultural identity.\"\n",
    "# Topic 5: \"Historical periods, political shifts, regional influence.\"\n",
    "# Topic 6: \"Creative expressions, entertainment milestones, cultural symbols.\"\n",
    "# Topic 7: \"Leadership roles, cultural symbols, national identity.\"\n",
    "# Topic 8: \"Cultural touchstones, historical landmarks, identity expressions.\"\n",
    "# Topic 9: \"Language and leadership, entertainment milestones, historical events.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting topic predictions for each question using the 10-topic model\n",
    "docsVStopics10 = lda_10.transform(tf)\n",
    "docsVStopics10 = pd.DataFrame(docsVStopics10, columns=[str(i+1) for i in range(10)])\n",
    "most_likely_topics10 = docsVStopics10.idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     29316\n",
       "10    19103\n",
       "2     21641\n",
       "3     22297\n",
       "4     18433\n",
       "5     18374\n",
       "6     18579\n",
       "7     20231\n",
       "8     19794\n",
       "9     18615\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show which topics have the most questions associated with them\n",
    "most_likely_topics10.groupby(most_likely_topics10).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add topic prediction labels for each question in the corpus and export labeled dataframe to csv\n",
    "df['10_topic_predictions'] = most_likely_topics10\n",
    "df.to_csv('JeopardyQuestions_Labeled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other resources used\n",
    "# https://www.geeksforgeeks.org/multiclass-classification-using-scikit-learn/\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html\n",
    "# https://stackoverflow.com/questions/35252762/finding-number-of-documents-per-topic-for-lda-with-scikit-learn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
